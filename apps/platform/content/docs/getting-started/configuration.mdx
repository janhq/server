---
title: Configuration
description: Configure Jan Server for your environment
full: false
---

# Configuration

Jan Server uses a flexible configuration system that works across development and production environments.

## Configuration Sources

Configuration is loaded in this order (later overrides earlier):

1. **YAML Defaults** - Built-in sensible defaults (`config/defaults.yaml`)
2. **Environment Files** - Environment-specific settings (`.env` files)
3. **Environment Variables** - Highest priority (great for secrets and deployment)

## Quick Setup

### Interactive Configuration

The easiest way to configure Jan Server:

```bash
make quickstart
```

This wizard will:
- Prompt for LLM provider (local vLLM or remote API)
- Configure MCP search tools (Serper, SearXNG, or none)
- Set up Media API (enable/disable)
- Generate `.env` and `config/secrets.env` files
- Start all services

### Manual Configuration

If you prefer manual setup:

```bash
# 1. Copy templates
cp .env.template .env
cp config/secrets.env.example config/secrets.env

# 2. Edit configuration files
nano .env
nano config/secrets.env

# 3. Validate configuration
make setup

# 4. Start services
make up-full
```

## Configuration Files

### `.env` (Main Configuration)

Generated by `make quickstart` or copied from `.env.template`:

```bash
# Environment
ENVIRONMENT=development

# LLM Provider
LLM_PROVIDER=vllm  # or openai, anthropic, groq
VLLM_PORT=8101

# MCP Tools
MCP_SEARCH_PROVIDER=serper  # or searxng, none

# Media API
MEDIA_API_ENABLED=true

# Service Ports
KONG_PORT=8000
LLM_API_PORT=8080
RESPONSE_API_PORT=8082
MCP_TOOLS_PORT=8091
MEDIA_API_PORT=8285
```

### `config/secrets.env` (Sensitive Data)

Store API keys and secrets (git-ignored):

```bash
# HuggingFace Token (for local vLLM)
HF_TOKEN=hf_xxxxxxxxxxxxx

# Remote LLM API Keys
OPENAI_API_KEY=sk-xxxxxxxxxxxxx
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx
GROQ_API_KEY=gsk_xxxxxxxxxxxxx

# MCP Search API Keys
SERPER_API_KEY=xxxxxxxxxxxxx

# AWS S3 (for Media API)
AWS_ACCESS_KEY_ID=xxxxxxxxxxxxx
AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxx
S3_BUCKET_NAME=jan-media
S3_REGION=us-east-1

# Database
DB_PASSWORD=secure_password_here
```

### `config/defaults.yaml` (System Defaults)

Base configuration with sensible defaults:

```yaml
environment: development

services:
  llm-api:
    http:
      port: 8080
      timeout: 30s
    database:
      dsn: postgres://jan_user:jan_password@api-db:5432/jan_llm_api
      max_idle_conns: 10
      max_open_conns: 30
    auth:
      enabled: true
      issuer: http://localhost:8085/realms/jan
```

## Environment Variables

Override any setting via environment variables (highest priority):

```bash
# Override HTTP port
export LLM_API_HTTP_PORT=9090

# Override database connection
export LLM_API_DATABASE_DSN=postgres://user:pass@prod-db:5432/db

# Enable observability
export LLM_API_OBSERVABILITY_ENABLED=true
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
```

## Service-Specific Configuration

### LLM API

```bash
# HTTP Server
LLM_API_HTTP_PORT=8080
LLM_API_HTTP_TIMEOUT=30s

# Database
LLM_API_DATABASE_DSN=postgres://user:pass@host:5432/db
LLM_API_DATABASE_MAX_IDLE_CONNS=10
LLM_API_DATABASE_MAX_OPEN_CONNS=30

# Authentication
LLM_API_AUTH_ENABLED=true
LLM_API_KEYCLOAK_BASE_URL=http://keycloak:8085
LLM_API_JWKS_URL=http://keycloak:8085/realms/jan/protocol/openid-connect/certs
LLM_API_ISSUER=http://localhost:8090/realms/jan

# Logging
LLM_API_LOG_LEVEL=info  # debug, info, warn, error
LLM_API_LOG_FORMAT=json  # json or text

# Observability (optional)
LLM_API_OBSERVABILITY_ENABLED=false
LLM_API_OTEL_SERVICE_NAME=llm-api
LLM_API_OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
```

### Response API

```bash
# HTTP Server
RESPONSE_API_HTTP_PORT=8082

# Tool Execution
RESPONSE_API_MAX_DEPTH=8  # Maximum tool chain depth
RESPONSE_API_TIMEOUT=120s  # Tool execution timeout

# MCP Tools
RESPONSE_API_MCP_URL=http://mcp-tools:8091
```

### Media API

```bash
# HTTP Server
MEDIA_API_HTTP_PORT=8285

# S3 Storage
MEDIA_API_AWS_ACCESS_KEY_ID=xxxxx
MEDIA_API_AWS_SECRET_ACCESS_KEY=xxxxx
MEDIA_API_S3_BUCKET_NAME=jan-media
MEDIA_API_S3_REGION=us-east-1

# Upload Limits
MEDIA_API_MAX_FILE_SIZE=10485760  # 10MB in bytes
```

### MCP Tools

```bash
# HTTP Server
MCP_TOOLS_PORT=8091

# Search Provider
MCP_SEARCH_PROVIDER=serper  # serper, searxng, or none
SERPER_API_KEY=xxxxx  # Required if using Serper

# SearXNG (if using local search)
SEARXNG_URL=http://searxng:8080

# Vector Store
VECTOR_STORE_URL=http://vector-store:3015
```

### Kong Gateway

```bash
# Gateway Port
KONG_PORT=8000

# Keycloak Integration
KEYCLOAK_BASE_URL=http://keycloak:8085
KEYCLOAK_REALM=jan
```

## LLM Provider Configuration

### Option 1: Local vLLM (GPU)

```bash
# In .env
LLM_PROVIDER=vllm
VLLM_PORT=8101

# In config/secrets.env
HF_TOKEN=hf_xxxxxxxxxxxxx
```

**Model Configuration:**
```bash
# Default model
VLLM_MODEL=Qwen/Qwen2.5-0.5B-Instruct

# GPU settings
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_MODEL_LEN=4096
```

### Option 2: OpenAI

```bash
# In .env
LLM_PROVIDER=openai
OPENAI_BASE_URL=https://api.openai.com/v1

# In config/secrets.env
OPENAI_API_KEY=sk-xxxxxxxxxxxxx
```

### Option 3: Anthropic

```bash
# In .env
LLM_PROVIDER=anthropic
ANTHROPIC_BASE_URL=https://api.anthropic.com

# In config/secrets.env
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx
```

### Option 4: Groq

```bash
# In .env
LLM_PROVIDER=groq
GROQ_BASE_URL=https://api.groq.com/openai/v1

# In config/secrets.env
GROQ_API_KEY=gsk_xxxxxxxxxxxxx
```

### Option 5: Azure OpenAI

```bash
# In .env
LLM_PROVIDER=azure
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_DEPLOYMENT=your-deployment-name

# In config/secrets.env
AZURE_OPENAI_API_KEY=xxxxxxxxxxxxx
```

## Database Configuration

### PostgreSQL

```bash
# Connection String
DB_DSN=postgres://user:password@host:5432/database?sslmode=disable

# Connection Pool
DB_MAX_IDLE_CONNS=10
DB_MAX_OPEN_CONNS=30
DB_CONN_MAX_LIFETIME=5m
DB_CONN_MAX_IDLE_TIME=10m
```

### Keycloak (Authentication)

```bash
# Keycloak Server
KEYCLOAK_BASE_URL=http://keycloak:8085
KEYCLOAK_REALM=jan

# Admin Credentials
KEYCLOAK_ADMIN=admin
KEYCLOAK_ADMIN_PASSWORD=admin

# Database
KEYCLOAK_DB_VENDOR=postgres
KEYCLOAK_DB_ADDR=api-db:5432
KEYCLOAK_DB_DATABASE=keycloak
KEYCLOAK_DB_USER=keycloak_user
KEYCLOAK_DB_PASSWORD=keycloak_password
```

## Observability Configuration

### Enable Monitoring

```bash
# Start monitoring stack
make monitor-up
```

### OpenTelemetry

```bash
# Enable OTEL
OTEL_ENABLED=true
OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317

# Service names
LLM_API_OTEL_SERVICE_NAME=llm-api
RESPONSE_API_OTEL_SERVICE_NAME=response-api
MEDIA_API_OTEL_SERVICE_NAME=media-api
MCP_TOOLS_OTEL_SERVICE_NAME=mcp-tools
```

### Prometheus

```bash
# Prometheus endpoint
PROMETHEUS_URL=http://prometheus:9090
```

### Grafana

```bash
# Grafana access
GRAFANA_URL=http://localhost:3001
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin
```

### Jaeger

```bash
# Jaeger tracing
JAEGER_URL=http://localhost:16686
```

## Validation

### Check Configuration

```bash
# Validate all configuration
make config-validate

# Show current configuration
make config-show

# Export configuration as YAML
make config-export
```

### Test Configuration

```bash
# Check service health
make health-check

# View service logs
make logs

# Test specific service
make logs-llm-api
```

## Production Configuration

### Security Best Practices

1. **Use secrets management:**
   ```bash
   # Use environment variables for secrets
   export OPENAI_API_KEY=$(aws secretsmanager get-secret-value --secret-id openai-key --query SecretString --output text)
   ```

2. **Enable TLS/SSL:**
   ```bash
   KONG_SSL_CERT_PATH=/path/to/cert.pem
   KONG_SSL_KEY_PATH=/path/to/key.pem
   ```

3. **Restrict network access:**
   ```bash
   # Bind only to localhost for internal services
   LLM_API_HTTP_HOST=127.0.0.1
   ```

4. **Use strong database passwords:**
   ```bash
   # Generate secure password
   DB_PASSWORD=$(openssl rand -base64 32)
   ```

### Performance Tuning

```bash
# Increase connection pools for production
DB_MAX_IDLE_CONNS=50
DB_MAX_OPEN_CONNS=200

# Adjust timeouts
LLM_API_HTTP_TIMEOUT=60s
RESPONSE_API_TIMEOUT=300s

# Enable connection pooling
DB_CONN_MAX_LIFETIME=30m
DB_CONN_MAX_IDLE_TIME=10m
```

## Troubleshooting

### Configuration Errors

```bash
# Validate configuration
make config-validate

# Check for syntax errors
docker compose config
```

### Service Won't Start

```bash
# Check logs for configuration errors
make logs

# Verify environment variables
docker compose config | grep -A 5 "environment:"
```

### Database Connection Issues

```bash
# Test database connection
docker exec -it jan-server-api-db-1 psql -U jan_user -d jan_llm_api

# Check connection string
echo $DB_DSN
```

## Next Steps

- [Architecture Overview](/docs/architecture/overview) - Understand the system design
- [Deployment Guide](/docs/guides/deployment) - Deploy to production
- [Operations & Troubleshooting](/docs/guides/operations) - Set up observability and monitoring
- [Docker Compose Configuration](/docs/configuration/docker-compose) - Advanced Docker setup
- [Kubernetes Configuration](/docs/configuration/kubernetes) - Helm values and K8s deployment

## Reference

- [Configuration Overview](/docs/configuration/overview)
- [Environment Variable Mapping](/docs/configuration/environment-variables)
- [Docker Compose Integration](/docs/configuration/docker-compose)
- [Kubernetes Deployment](/docs/configuration/kubernetes)

## Need Help?

- [Operations & Troubleshooting](/docs/guides/operations)
- [GitHub Issues](https://github.com/janhq/jan-server/issues)
- [GitHub Discussions](https://github.com/janhq/jan-server/discussions)
