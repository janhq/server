# vLLM Inference Services
# GPU and CPU model inference

volumes:
  hf-cache:
  models-cache:

services:
  # vLLM GPU Inference (requires NVIDIA GPU)
  vllm-jan-gpu:
    image: vllm/vllm-openai:latest
    pull_policy: always
    restart: unless-stopped
    profiles: ["gpu", "full"]
    env_file:
      - ${ENV_FILE:-../.env}
    command: >
      --model ${VLLM_MODEL:-janhq/Jan-v1-4B}
      --host ${VLLM_HOST:-0.0.0.0}
      --port ${VLLM_PORT:-8101}
      --dtype ${VLLM_DTYPE:-float16}
      --api-key ${VLLM_API_KEY:-changeme}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTIL:-0.66}
      --enforce-eager
      --download-dir /models
    environment:
      # Hugging Face
      HF_TOKEN: ${HF_TOKEN}
      
      # Sampling Configuration
      VLLM_USE_FLASHINFER_SAMPLER: ${VLLM_USE_FLASHINFER_SAMPLER:-0}
      VLLM_DISABLE_FLASHINFER_PREFILL: ${VLLM_DISABLE_FLASHINFER_PREFILL:-1}
      
      # Attention Backend
      VLLM_ATTENTION_BACKEND: ${VLLM_ATTENTION_BACKEND:-TORCH_SDPA}
      
      # Compilation Configuration
      VLLM_USE_STANDALONE_COMPILE: ${VLLM_USE_STANDALONE_COMPILE:-0}
      VLLM_DISABLE_COMPILE_CACHE: ${VLLM_DISABLE_COMPILE_CACHE:-1}
      VLLM_TORCH_COMPILE: ${VLLM_TORCH_COMPILE:-0}
      
      # Flash Attention
      VLLM_USE_FLASH_ATTENTION: ${VLLM_USE_FLASH_ATTENTION:-0}
      VLLM_USE_FLASHINFER: ${VLLM_USE_FLASHINFER:-0}
    volumes:
      - hf-cache:/root/.cache/huggingface
      - models-cache:/models
    shm_size: "4g"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "-H", "Authorization: Bearer ${VLLM_INTERNAL_KEY:-changeme}", "http://localhost:8101/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 60
    ports:
      - "${VLLM_PORT:-8101}:${VLLM_PORT:-8101}"

  # vLLM CPU Inference (slower, no GPU required)
  vllm-jan-cpu:
    image: vllm/vllm-openai:latest
    pull_policy: always
    restart: unless-stopped
    profiles: ["cpu"]
    env_file:
      - ${ENV_FILE:-../.env}
    command: >
      --model ${VLLM_CPU_MODEL:-janhq/Jan-v1-4B}
      --served-model-name ${VLLM_CPU_SERVED_NAME:-jan-v1-4b}
      --host ${VLLM_HOST:-0.0.0.0}
      --port ${VLLM_PORT:-8101}
      --dtype ${VLLM_CPU_DTYPE:-float32}
      --api-key ${VLLM_API_KEY:-changeme}
      --download-dir /models
      --enforce-eager
    environment:
      # Hugging Face
      HF_TOKEN: ${HF_TOKEN}
      
      # Flash Attention (disabled for CPU)
      VLLM_USE_FLASHINFER: ${VLLM_USE_FLASHINFER:-0}
      VLLM_USE_FLASH_ATTENTION: ${VLLM_USE_FLASH_ATTENTION:-0}
    volumes:
      - hf-cache:/root/.cache/huggingface
      - models-cache:/models
    healthcheck:
      test: ["CMD", "curl", "-fsS", "-H", "Authorization: Bearer ${VLLM_INTERNAL_KEY:-changeme}", "http://localhost:8101/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 60
    ports:
      - "${VLLM_PORT:-8101}:${VLLM_PORT:-8101}"
